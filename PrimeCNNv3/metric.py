# AUTOGENERATED! DO NOT EDIT! File to edit: 05_metrics.ipynb (unless otherwise specified).

__all__ = ['accuracy', 'error_rate', 'precision', 'recall', 'f1score']

# Cell
from .imports import *
from sklearn.metrics import precision_score, recall_score, f1_score

# Cell
def accuracy(preds, target, dim = -1):
    '''
        Returns accuracy

        Args:
        preds (np.array): prediction of the model
        target (np.array): target values
        dim: dimention to use
    '''
    return (preds.argmax(dim = dim) == target).float().mean().item()

# Cell
def error_rate(preds, target, dim = -1):
    '''
        Returns error_rate
        1- accuracy

        Args:
        preds (torch tensor): prediction of the model
        target (torch tensor): target values
        dim: dimention to use
    '''
    return 1 - accuracy(preds=preds, target = target, dim = -1)

# Cell
def precision(preds, target, dim = -1):
    '''Global average precision score'''
    return precision_score(y_pred=preds.argmax(dim = dim).numpy(), y_true = target.numpy(), average='macro')

# Cell
def recall(preds, target, dim = -1):
    return recall_score(y_pred=preds.argmax(dim = dim).numpy(), y_true = target.numpy(), average='macro')

# Cell
def f1score(preds, target, dim = -1):
    return f1_score(y_pred=preds.argmax(dim = dim).numpy(), y_true = target.numpy(), average='macro')