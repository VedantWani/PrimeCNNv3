# AUTOGENERATED! DO NOT EDIT! File to edit: 05_metrics.ipynb (unless otherwise specified).

__all__ = ['accuracy', 'error_rate', 'precision', 'recall', 'f1score', 'confusion_matrix', 'plot_confusion_matrix']

# Cell
from .imports import *
from sklearn.metrics import precision_score, recall_score, f1_score
from sklearn.metrics import confusion_matrix as cm
import seaborn as sns

# Cell
def accuracy(preds, target, dim = -1):
    '''
        Returns accuracy

        Args:
        preds (np.array): prediction of the model
        target (np.array): target values
        dim: dimention to use
    '''
    return (preds.argmax(dim = dim) == target).float().mean().item()

# Cell
def error_rate(preds, target, dim = -1):
    '''
        Returns error_rate
        1- accuracy

        Args:
        preds (torch tensor): prediction of the model
        target (torch tensor): target values
        dim: dimention to use
    '''
    return 1 - accuracy(preds=preds, target = target, dim = -1)

# Cell
def precision(preds, target, dim = -1):
    '''Global average precision score'''
    return precision_score(y_pred=preds.argmax(dim = dim).numpy(), y_true = target.numpy(), average='macro')

# Cell
def recall(preds, target, dim = -1):
    return recall_score(y_pred=preds.argmax(dim = dim).numpy(), y_true = target.numpy(), average='macro')

# Cell
def f1score(preds, target, dim = -1):
    return f1_score(y_pred=preds.argmax(dim = dim).numpy(), y_true = target.numpy(), average='macro')

# Cell
def confusion_matrix(preds, target, labels = None, **kwargs):
    return cm(y_pred=preds, y_true=target,**kwargs)


# Cell
def plot_confusion_matrix(preds, target, labels = None, **kwargs):
    sns.heatmap(cm(y_pred=preds, y_true=target), annot = True, cmap="Blues", fmt='d', xticklabels=labels,yticklabels=labels, cbar = False )
    plt.xlabel('Predicted')
    plt.ylabel('Target')