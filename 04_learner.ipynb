{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leaner to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from PrimeCNNv3.imports import *\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from PrimeCNNv3.utils.vizualize import plot_lr\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (<ipython-input-1-e12f1ea2bffc>, line 221)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-e12f1ea2bffc>\"\u001b[1;36m, line \u001b[1;32m221\u001b[0m\n\u001b[1;33m    self.opt = self.opt_func(params = params, self.model.parameters()), lr = self.lr,\u001b[0m\n\u001b[1;37m                                                                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "class Learner:\n",
    "    '''Learner for the Model'''\n",
    "    def __init__(self, model, dls, metric, cbs, loss_func, opt_func = None, lr_schedular_func = None):\n",
    "        self.model = model\n",
    "        self.dls = dls\n",
    "        self.device = torch.device(dls.device)\n",
    "        self.metric = metric\n",
    "        self.cbs = cbs\n",
    "        self.loss_func = loss_func\n",
    "        self.opt_func = opt_func\n",
    "        self.lr_schedular_func = lr_schedular_func\n",
    "        \n",
    "        self.opt = None\n",
    "        self.lr_schedular = None\n",
    "        \n",
    "        #get the number of layer groups in the model\n",
    "        self.layer_groups = len([*self.model.children()])\n",
    "        \n",
    "        for cb in self.cbs:\n",
    "            cb.learner = self\n",
    "        \n",
    "    \n",
    "    def lr_finder(self, final_lr = 10, start_lr = 1e-08,num_iter = 100, wd = 0, beta = 0.98, suggestion = True):\n",
    "        \n",
    "        #save model, optimizer and lr_schdeular\n",
    "        saved_param = {\n",
    "            'model' : self.model.state_dict(),\n",
    "            'optimizer' : self.opt.state_dict() if self.opt is not None else None,\n",
    "            'lr_schedular' : self.lr_schedular if self.lr_schedular is not None else None\n",
    "        }\n",
    "        \n",
    "        torch.save(saved_param, 'tmp.pth')\n",
    "        \n",
    "        \n",
    "        store_cbs = self.cbs\n",
    "        self.cbs = []\n",
    "        avg_loss = 0.\n",
    "        best_loss = 0.\n",
    "        \n",
    "        lrs = []\n",
    "        lr_loss = []\n",
    "        self.num_iter = num_iter\n",
    "        self.model.to(self.device)\n",
    "        self.training = True\n",
    "    \n",
    "        self._init_optimizer(epochs = None, lr= start_lr, wd = wd)\n",
    "        self.lr_schedular = ExponentialLR(self.opt,final_lr= final_lr, num_iter= self.num_iter)\n",
    "        \n",
    "        for self.num,batch in  enumerate(progress_bar(self.dls.train, leave = False)):\n",
    "            xb, yb = batch\n",
    "            #xb,yb = iter(self.dls.train).next()\n",
    "            self.xb = xb.to(self.device, non_blocking = True)\n",
    "            self.yb = yb.to(self.device, non_blocking = True)\n",
    "            \n",
    "            #do one batch\n",
    "            self.do_batch()\n",
    "            \n",
    "            #take moving average\n",
    "            avg_loss = avg_loss * beta + (1-beta) * self.running_loss\n",
    "            smooth_loss = avg_loss / (1 - beta**(self.num + 1))\n",
    "            \n",
    "            if self.num + 1 > 1 and smooth_loss > 4 * best_loss:\n",
    "                print('Loss exploding.. stopping training')\n",
    "                break\n",
    "            \n",
    "            if smooth_loss < best_loss or self.num + 1 == 1:\n",
    "                best_loss = smooth_loss\n",
    "                \n",
    "            if self.num == self.num_iter:\n",
    "                break\n",
    "            #refactor later\n",
    "            lr_loss.append(smooth_loss)\n",
    "            lrs.append(self.lr_schedular.get_last_lr())\n",
    "        \n",
    "        if suggestion:\n",
    "            lr_s, losses = torch.tensor(lrs[num_iter // 10 : -5]), torch.tensor(lr_loss[num_iter // 10 :-5])\n",
    "            if not (len(losses) == 0):\n",
    "                lr_min = lr_s[losses.argmin()].item()\n",
    "                grad = (losses[1:] - losses[:-1]) / (lr_s[1:].log() - lr_s[:-1].log())\n",
    "                \n",
    "                lr_steep = lr_s[grad.argmin()].item()\n",
    "                \n",
    "                print(f'Suggested LR : \\nmin_lr: {lr_min / 10} \\nlr_steep: {lr_steep}' )\n",
    "            else:\n",
    "                print(f'Length of Loss is zero, NO suggestion')\n",
    "        self.xb, self.yb = (None,),(None,)\n",
    "        self.preds = None\n",
    "        self.loss = None\n",
    "        \n",
    "        plot_lr(lrs, lr_loss)\n",
    "        \n",
    "        self.cbs = store_cbs\n",
    "        saved_param = torch.load('tmp.pth')\n",
    "        self.model.load_state_dict(saved_param['model'])\n",
    "        self.xb, self.yb = None, None\n",
    "        #if saved_param['optimizer'] is None:\n",
    "            #self.opt = None\n",
    "        #else:\n",
    "            #self.opt.load_state_dict(saved_param['optimizer'])\n",
    "        \n",
    "        if saved_param['lr_schedular'] is None:\n",
    "            self.lr_schedular = None\n",
    "        else:\n",
    "            self.lr_schedular = saved_param['lr_schedular']\n",
    "            \n",
    "        \n",
    "    \n",
    "    def do_batch(self):\n",
    "        self('before_batch')\n",
    "        self.batch_size = len(self.yb)\n",
    "        self.preds = self.model(self.xb)\n",
    "        self.loss = self.loss_func(self.preds, self.yb)\n",
    "        self.running_loss = self.loss.item()\n",
    "        \n",
    "        if self.training:\n",
    "            self('before_step')\n",
    "            self.opt.zero_grad()\n",
    "            self.loss.backward()\n",
    "            self.opt.step()\n",
    "            self('after_step')\n",
    "            \n",
    "            if self.lr_schedular is not None:\n",
    "                self.lr_schedular.step()\n",
    "                \n",
    "        self('after_batch')\n",
    "      \n",
    "    \n",
    "    def do_valid_epoch(self):\n",
    "        self('before_valid_epoch')\n",
    "    \n",
    "        dl = self.dls.valid\n",
    "        \n",
    "        for self.num, self.batch in enumerate(progress_bar(dl, parent= self.mb)):\n",
    "            with torch.no_grad():\n",
    "                self.do_batch()\n",
    "                \n",
    "        self('after_valid_epoch')\n",
    "   \n",
    "    def do_train_epoch(self):\n",
    "        self('before_train_epoch')\n",
    "        \n",
    "        dl = self.dls.train\n",
    "        for self.num, self.batch in enumerate(progress_bar(dl, parent= self.mb)):\n",
    "            self.do_batch()\n",
    "            \n",
    "        self('after_train_epoch')\n",
    "    \n",
    "    \n",
    "    def fit(self, epochs, lr, wd, **kwargs):\n",
    "        \n",
    "        \n",
    "        self._init_optimizer(epochs, lr, wd, **kwargs)\n",
    "        \n",
    "        #if self.lr_schedular_func is not None:\n",
    "            #only for one cycle (Note: can be set at before_fit callback)\n",
    "           # self.lr_schedular = torch.optim.lr_scheduler.OneCycleLR(self.opt, max_lr = lr, epochs = epochs,\n",
    "                                                                #steps_per_epoch = len(self.dls.train))\n",
    "        self('before_fit')\n",
    "        try:\n",
    "          \n",
    "            for self.epoch in self.mb:\n",
    "                self('before_epoch')\n",
    "                self.do_train_epoch()\n",
    "                self.do_valid_epoch()\n",
    "                self('after_epoch')\n",
    "\n",
    "        except FitCancelException: pass\n",
    "        \n",
    "        self('after_fit')\n",
    "        \n",
    "    def fine_tune(self, epochs, max_lr, wd, freeze_epoch = 3, initial_freeze = 7, div_factor = 15., pct_start= 0.25,final_div_factor =1e4,lr_div = 10, **kwargs):\n",
    "        '''\n",
    "            Initially freezes layers last layers i.e fully connected layer based on initial freeze value.\n",
    "            Trains those layer with one_cycle_fit policy for 5 epochs, later, unfreezes all the layers, \n",
    "            train for given number of epochs\n",
    "            \n",
    "            initial freeze as per resnet\n",
    "        '''\n",
    "        self.freeze_to(freeze_to_till = initial_freeze,show_log = True)\n",
    "        self.one_cycle_fit(epochs = freeze_epoch, max_lr = max_lr, wd = wd, div_factor = div_factor, pct_start = pct_start, final_div_factor = final_div_factor, **kwargs)\n",
    "        \n",
    "        self.unfreeze()\n",
    "        \n",
    "        self.one_cycle_fit(epochs = epochs, max_lr = max_lr / lr_div, wd = wd, div_factor = div_factor, pct_start = pct_start, final_div_factor = final_div_factor, **kwargs)\n",
    "        \n",
    "    def one_cycle_fit(self, epochs, max_lr, wd, div_factor = 15.0, pct_start= 0.25,final_div_factor =1e4, **kwargs):\n",
    "        #div_factor and pct changed from default\n",
    "    \n",
    "        self._init_optimizer(epochs, max_lr, wd, **kwargs)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.lr_schedular = torch.optim.lr_scheduler.OneCycleLR(self.opt, max_lr = max_lr, epochs = epochs,\n",
    "                                                                steps_per_epoch = len(self.dls.train), div_factor= div_factor,\n",
    "                                                           pct_start = pct_start, final_div_factor = final_div_factor)\n",
    "        self('before_fit')\n",
    "        try:\n",
    "            \n",
    "            for self.epoch in self.mb:\n",
    "                self('before_epoch')\n",
    "                self.do_train_epoch()\n",
    "                self.do_valid_epoch()\n",
    "                self('after_epoch')\n",
    "  \n",
    "        except FitCancelException: pass\n",
    "        \n",
    "       \n",
    "        \n",
    "        self('after_fit')\n",
    "    \n",
    "    def _init_optimizer(self, epochs, lr, wd,params = None **kwargs):\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.wd = wd\n",
    "        \n",
    "        if params is None:\n",
    "            params = filter(lambda p: p.requires_grad)\n",
    "            \n",
    "        #eps is changed to 1e-05 from default value of 1e-08\n",
    "        self.opt = self.opt_func(params = params, self.model.parameters()), lr = self.lr,\n",
    "                                weight_decay = self.wd, **kwargs)\n",
    "    \n",
    "    def freeze_to(self, freeze_to_till, show_log = True):\n",
    "        '''\n",
    "            change require grad to false i.e freeze the layers\n",
    "        '''\n",
    "        if freeze_to_till < self.layer_groups or freeze_to_till > -self.layer_groups:\n",
    "\n",
    "            freeze_to_till = freeze_to_till if freeze_to_till >= 0 else (self.layer_groups) + freeze_to_till\n",
    "\n",
    "            for idx, layer in enumerate(self.model.children()):\n",
    "                if idx >= freeze_to_till:\n",
    "                    if show_log:\n",
    "                        print(f'Layer :{idx} unFreeze')\n",
    "                    for param in layer.parameters():\n",
    "                        param.requires_grad = True\n",
    "\n",
    "                else:\n",
    "                    if show_log:\n",
    "                        print(f'Layer :{idx} Freeze')\n",
    "                    for param in layer.parameters():\n",
    "                        param.requires_grad = False\n",
    "                        \n",
    "    def freeze(self):\n",
    "        '''\n",
    "            Freeze last group of layers (in this case classifier)\n",
    "        '''\n",
    "        self.freeze_to(freeze_to_till=-1)\n",
    "        \n",
    "    def unfreeze(self):\n",
    "        self.freeze_to(freeze_to_till=0, show_log = False)\n",
    "        \n",
    "    def __call__(self, name):\n",
    "        for cb in self.cbs:\n",
    "            getattr(cb,name,noop)()\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        '''\n",
    "        save the model\n",
    "        '''\n",
    "        torch.save(self.model.state_dict(), path)\n",
    "   \n",
    "    def load_model(self, path):\n",
    "        '''\n",
    "        load the model wieghts\n",
    "        '''\n",
    "        self.model.load_state_dict(torch.load(path))\n",
    "        \n",
    "    def predict(self, image, classes:list = None):\n",
    "        '''args:\n",
    "                input:\n",
    "                    image(PIL or numpy array)\n",
    "                    return the model prediction class\n",
    "                    \n",
    "        '''\n",
    "        if isinstance(image,Image.Image):\n",
    "            image_data = np.array(image.convert('RGB'))\n",
    "        if isinstance(image, np.ndarray):\n",
    "            assert len(image.shape) == 3, 'Image is does not have 3 channels'\n",
    "            image_data = image\n",
    "            \n",
    "        transform = A.Compose([A.Resize(224,224),A.Normalize(), ToTensorV2()])\n",
    "        image_tensor = transform(image = image_data)['image'].unsqueeze(dim = 0)\n",
    "        image_tensor = image_tensor.to(self.device)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        preds = self.model(image_tensor).cpu().detach()\n",
    "        \n",
    "        if hasattr(self.dls.valid.dataset,'CLASSES'):\n",
    "            labels = list(self.dls.valid.dataset.CLASSES.key())\n",
    "        elif classes is not None:\n",
    "            labels = classes\n",
    "        \n",
    "        \n",
    "        return {'Class':labels[preds.argmax(dim = -1).item()], 'Propability': preds}\n",
    "        \n",
    "        \n",
    "    def get_preds(self, dl):\n",
    "        '''input: \n",
    "            dataloader\n",
    "            for each batch in dataloader\n",
    "                get prediction from the model store it.\n",
    "            \n",
    "            after going through whole data from dl\n",
    "            return prediction\n",
    "        '''\n",
    "        preds = []\n",
    "        target = []\n",
    "        \n",
    "        for xb,yb in dl:\n",
    "            \n",
    "            xb = xb.to(self.device, non_blocking = True)\n",
    "            yb = yb.to(self.device, non_blocking = True)\n",
    "            self.model.to(self.device)\n",
    "            self.model.eval()\n",
    "            pred = self.model(xb).cpu().detach()\n",
    "            \n",
    "            preds.append(pred.argmax(dim = -1))\n",
    "            if yb is not None:\n",
    "                target.append(yb.cpu().detach())\n",
    "            \n",
    "        return torch.cat(preds),torch.cat(target)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ExponentialLR(_LRScheduler):\n",
    "    ''' \n",
    "        ExponentialLR for LR range finder\n",
    "    '''\n",
    "    def __init__(self, opt, final_lr,num_iter, last_epoch = -1):\n",
    "        '''\n",
    "            ExponentialLR\n",
    "            \n",
    "            Args:\n",
    "                opt (optimizer) : optimizer to use\n",
    "                final_lr(float) : max lr bound\n",
    "                num_iter(int) : number of iterations\n",
    "                last_epoch (int): -1\n",
    "        '''\n",
    "        \n",
    "        self.final_lr = final_lr\n",
    "        self.num_iter = num_iter\n",
    "        assert num_iter > 1, 'number of iteration must be grater than 1'\n",
    "        \n",
    "        super(ExponentialLR, self).__init__(opt, last_epoch=last_epoch)\n",
    "    \n",
    "    def get_lr(self):\n",
    "        r = self.last_epoch / (self.num_iter - 1)\n",
    "        \n",
    "        return [base_lr * ((self.final_lr / base_lr)**r) for base_lr in self.base_lrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def random_seed(seed):\n",
    "    '''Sets the seed value for reproducibility'''\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def seed_worker(worker_id):\n",
    "    '''\n",
    "        for dataloader reproducibility and avoiding duplicates\n",
    "    '''\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class FitCancelException(Exception):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_utils.data.dataset.ipynb.\n",
      "Converted 01_utils.data.dataloaders.ipynb.\n",
      "Converted 02_utils.vizualize.ipynb.\n",
      "Converted 03_callbacks.ipynb.\n",
      "Converted 03a_hooks.ipynb.\n",
      "Converted 04_learner.ipynb.\n",
      "Converted 05_metrics.ipynb.\n",
      "Converted 06_NIA.WOA.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
